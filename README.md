# LumTokenizer 1.0.6.2

> A faster, lighter tokenizer library for .NET (.NET 10)
---

## âœ¨ Whatâ€™s New (2025 Refresh)

| Feature | Status | Description |
|---------|--------|-------------|
| **External vocab** | âœ… | Load custom `tokenizer.json` at runtimeâ€”no recompilation, no hard-coded maps. |
| **Special tokens** | âœ… | Add special tokens supported int tokenizer.json; keep or hide `<|endoftext|>` etc. during decode. |
| **Chinese-safe optimized** | âœ… | For a better performance. |
| **Unit tests** | âœ… | Added core/robustness/Chinese/English/mixed unit test cases. |
| **Highly efficient** | âœ… | Highly efficient tokenization is achieved through a HighPerformanceSpanSplitter and a span-based dictionary purpose-built for speed. |


## ğŸš€ Quick Start

### 1. Install
```bash
dotnet add package LumTokenizer
```

### 2. The encoding and decoding
```csharp

var tokenizer = BPETokenizer.CreateTokenizer("minimind_tokenizer.txt"); // Not thread safe.
var ctokenizer = ConcurrentBPETokenizer.CreateTokenizer("minimind_tokenizer.txt"); // Thread safe.

Console.WriteLine(tokenizer.VocabSize);

var ids = tokenizer.Encode("hello!  è¤ç«åˆèŠ’ï¼Œä½ å¥½");

Console.WriteLine(string.Join(",",ids));

Console.WriteLine(tokenizer.Decode(ids));

// Output
// 6400
// 5125,338,3,223,223,1109,100,2399,3187,784,243,270,5134
// hello!  è¤ç«åˆèŠ’ï¼Œä½ å¥½

// More with minimind_tokenizer 6400
// hello friend => 5125,338,2487
// ä¸­å›½åŒ—äº¬=> 2366,6210
// <|im_start|>i'm å¥½<|im_end|> => 1,75,2115,223,587,2

// More with qwen 151643
// ä¸­å›½åŒ—äº¬ => 58695,68990
// hello friend => 14990,4238
// <|im_start|>i'm å¥½<|im_end|> => 151644,72,2776,4891,98,121,151645
```

## Comparison

Here is a comparison of token IDs generated by LumTokenizer, SharpToken, and TikTokenr for the same input text using different tokenizers.

```
LumTokenizer_cl100k_base
34655,61078,11,832,315,42482,596,77069,323,1455,73135,11335,11,10975,279,3446,315,279,46337,323,12280,12970,61078,11,889,65928,813,26135,11,439,568,1587,813,3611,38705,11,4184,311,52671,323,74571,13,61078,753,8060,439,264,7126,2995,360,3933,5678,323,813,1917,304,63355,323,31926,16134,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,1822,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,1822,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,1822,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,1822,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,1822,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,1822,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,1822,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,29
King Lear, one of Shakespeare's darkest and most savage plays, tells the story of the foolish and Job-like Lear, who divides his kingdom, as he does his affections, according to vanity and whim. Learâ€™s failure as a father engulfs himself and his world in turmoil and tragedy.<|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|>

SharpToken_cl100k_base
34655,61078,11,832,315,42482,596,77069,323,1455,73135,11335,11,10975,279,3446,315,279,46337,323,12280,12970,61078,11,889,65928,813,26135,11,439,568,1587,813,3611,38705,11,4184,311,52671,323,74571,13,61078,753,8060,439,264,7126,2995,360,3933,5678,323,813,1917,304,63355,323,31926,16134,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,1822,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,1822,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,1822,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,1822,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,1822,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,1822,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,1822,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,29
King Lear, one of Shakespeare's darkest and most savage plays, tells the story of the foolish and Job-like Lear, who divides his kingdom, as he does his affections, according to vanity and whim. Learâ€™s failure as a father engulfs himself and his world in turmoil and tragedy.<|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|>

TikTokenr_cl100k_base
34655,61078,11,832,315,42482,596,77069,323,1455,73135,11335,11,10975,279,3446,315,279,46337,323,12280,12970,61078,11,889,65928,813,26135,11,439,568,1587,813,3611,38705,11,4184,311,52671,323,74571,13,61078,753,8060,439,264,7126,2995,360,3933,5678,323,813,1917,304,63355,323,31926,16134,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,1822,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,1822,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,1822,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,1822,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,1822,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,1822,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,1822,91,318,5011,91,29,15339,220,220,57668,53901,27,91,318,6345,91,29
King Lear, one of Shakespeare's darkest and most savage plays, tells the story of the foolish and Job-like Lear, who divides his kingdom, as he does his affections, according to vanity and whim. Learâ€™s failure as a father engulfs himself and his world in turmoil and tragedy.<|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|>


LumTokenizer_qwen150k
33555,59978,11,825,315,41382,594,75969,323,1429,72035,11088,11,10742,279,3364,315,279,45237,323,12011,12681,59978,11,879,64828,806,25079,11,438,566,1558,806,3527,37605,11,4092,311,51571,323,73471,13,59978,748,7901,438,264,6981,2922,360,3848,5561,323,806,1879,304,62255,323,30826,13,151644,14990,220,220,108386,151645,151644,14990,220,220,108386,151645,151644,14990,220,220,108386,151645,151644,14990,220,220,108386,151645,151644,14990,220,220,108386,151645,151644,14990,220,220,108386,151645,151644,14990,220,220,108386,151645,151644,14990,220,220,108386,151645
King Lear, one of Shakespeare's darkest and most savage plays, tells the story of the foolish and Job-like Lear, who divides his kingdom, as he does his affections, according to vanity and whim. Learâ€™s failure as a father engulfs himself and his world in turmoil and tragedy.<|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|><|im_start|>hello  ä½ å¥½<|im_end|>
```
## Benchmark



| Method                    | text                 | Mean      | Error    | StdDev   | Ratio | RatioSD | Gen0   | Allocated | Alloc Ratio |
|-------------------------- |--------------------- |----------:|---------:|---------:|------:|--------:|-------:|----------:|------------:|
| SharpToken_cl100k_base    | Chinese  | 122.99 us | 2.314 us | 2.273 us |  5.71 |    0.12 | 0.7324 |    9.1 KB |        1.19 |
| TiktokenSharp_cl100k_base | Chinese  |  96.00 us | 1.829 us | 2.106 us |  4.45 |    0.11 | 0.4883 |   6.34 KB |        0.83 |
| LumTokenizer_cl100k_base  | Chinese  |  21.56 us | 0.268 us | 0.251 us |  1.00 |    0.02 | 0.6104 |   7.63 KB |        1.00 |
|                           |                      |           |          |          |       |         |        |           |             |
| SharpToken_cl100k_base    | English |  26.77 us | 0.520 us | 0.639 us |  1.02 |    0.03 | 0.6714 |   8.38 KB |        0.74 |
| TiktokenSharp_cl100k_base | English |  20.21 us | 0.383 us | 0.376 us |  0.77 |    0.02 | 0.4272 |   5.51 KB |        0.49 |
| LumTokenizer_cl100k_base  | English |  26.13 us | 0.495 us | 0.509 us |  1.00 |    0.03 | 0.9155 |  11.31 KB |        1.00 |
|                           |                      |           |          |          |       |         |        |           |             |
| SharpToken_cl100k_base    | Mixed  |  90.97 us | 1.580 us | 1.478 us |  3.78 |    0.09 | 0.8545 |   10.9 KB |        1.23 |
| TiktokenSharp_cl100k_base | Mixed  |  63.85 us | 1.274 us | 1.564 us |  2.65 |    0.08 | 0.4883 |   6.74 KB |        0.76 |
| LumTokenizer_cl100k_base  | Mixed  |  24.08 us | 0.465 us | 0.435 us |  1.00 |    0.03 | 0.7019 |   8.83 KB |        1.00 |


Special tokens were consistently ignored under default settings.
SharpToken is at version 2.0.4; TiktokenSharp is at 1.2.0.
The following is the benchmark code used:

```csharp

internal class Program
    {
        static void Main(string[] args)
        {           
            BenchmarkRunner.Run<CompareBenchmark>();
        }
    }

    [MemoryDiagnoser]
    public class CompareBenchmark
    {
        internal GptEncoding _sharpToken;
        internal TikToken _tikToken;
        internal BPETokenizer _tokenizer1;
        internal BPETokenizer _tokenizer2;

        [GlobalSetup]
        public void Setup()
        {
            _sharpToken = GptEncoding.GetEncoding("cl100k_base");
            _tikToken = TikToken.GetEncodingAsync("cl100k_base").ConfigureAwait(false).GetAwaiter().GetResult();
            _tokenizer1 = BPETokenizer.CreateTokenizer(
                @"D:\Data\Personal\AI\llm\tokenizer\cl100k.txt", true, RegexType.RegexCl100KBase);
            _tokenizer2 = BPETokenizer.CreateTokenizer(
                @"D:\Data\Personal\AI\llm\tokenizer\qw_tokenizer.json", false, RegexType.RegexCl100KBase);
        }

        // ====== 1. å£°æ˜å‚æ•°æº ======
        public IEnumerable<string> TextSamples()
        {
            yield return TextCatalog.English;
            yield return TextCatalog.Chinese;
            yield return TextCatalog.Mixed;
        }

        // ====== 2. æ¯ä¸ªæ–¹æ³•æ”¹æˆå¸¦å‚æ•° ======
        [Benchmark]
        [ArgumentsSource(nameof(TextSamples))]
        public int SharpToken_cl100k_base(string text)
        {
            var encoded = _sharpToken.Encode(text);
            var decoded = _sharpToken.Decode(encoded);
            return encoded.Count;
        }

        [Benchmark]
        [ArgumentsSource(nameof(TextSamples))]
        public int TiktokenSharp_cl100k_base(string text)
        {
            var encoded = _tikToken.Encode(text);
            var decoded = _tikToken.Decode(encoded);
            return encoded.Count;
        }

        [Benchmark(Baseline =true)]
        [ArgumentsSource(nameof(TextSamples))]
        public int LumTokenizer_cl100k_base(string text)
        {
            var encoded = _tokenizer1.Encode(text, false);
            var decoded = _tokenizer1.Decode(encoded, false);
            return encoded.Count;
        }
              
        public int LumTokenizer_qwen150k(string text)
        {
            var encoded = _tokenizer2.Encode(text, false);
            var decoded = _tokenizer2.Decode(encoded, false);
            return encoded.Count;
        }
    }
    public static class TextCatalog
    {
        /* 1 è‹±æ–‡é•¿å¯¹è¯ */
        public static readonly string English =
            "Human: Can you explain how gradient descent works in deep learning?\n\n" +
            "Assistant: Sure! Gradient descent is an optimization algorithm used to minimize the loss function. " +
            "The basic idea is to compute the gradient of the loss with respect to each parameter, then update " +
            "the parameters in the opposite direction of the gradient. The learning rate controls the step size. " +
            "There are variants like SGD, momentum, Adam, each improving convergence speed or stability. " +
            "In practice, we use mini-batch gradient descent to balance computational efficiency and convergence. " +
            "The loss landscape can be very high-dimensional and non-convex, so careful tuning of hyper-parameters " +
            "such as learning rate schedules, weight decay, and initialization strategies is essential. " +
            "Without these tricks, training can stall or diverge.\n\n" +
            "Human: What are the common tricks to avoid overfitting?\n\n" +
            "Assistant: Common regularization techniques include dropout, weight decay (L2), early stopping, " +
            "data augmentation, and batch normalization. Increasing dataset size and using simpler models also help.";

        /* 2 çº¯ä¸­æ–‡é•¿å¯¹è¯ */
        public static readonly string Chinese =
            "äººç±»ï¼šè¯·è¯¦ç»†ä»‹ç»ä¸€ä¸‹ Transformer çš„æ ¸å¿ƒæ€æƒ³ã€‚\n\n" +
            "åŠ©æ‰‹ï¼šTransformer å®Œå…¨æ‘’å¼ƒäº†é€’å½’ç»“æ„ï¼Œä»…ä¾é è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥æ•æ‰åºåˆ—ä¸­çš„é•¿è·ç¦»ä¾èµ–ã€‚ " +
            "è¾“å…¥åºåˆ—é¦–å…ˆè¢«æ˜ å°„ä¸ºæŸ¥è¯¢ã€é”®å’Œå€¼ä¸‰ä¸ªå‘é‡ï¼Œæ¥ç€é€šè¿‡ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›è®¡ç®—æ¯ä¸€ä½ç½®å¯¹å…¶ä»–ä½ç½®çš„æƒé‡ï¼Œ " +
            "ä»è€Œåœ¨ä¸€æ¬¡å‰å‘ä¼ æ’­ä¸­åŒæ—¶èšåˆå…¨å±€ä¿¡æ¯ã€‚å¤šå¤´æœºåˆ¶å…è®¸æ¨¡å‹åœ¨ä¸åŒå­ç©ºé—´å†…å¹¶è¡Œå­¦ä¹ å¤šç§å…³ç³»ã€‚ " +
            "æ­¤å¤–ï¼Œä½ç½®ç¼–ç è¢«ç›´æ¥åŠ åˆ°è¯å‘é‡ä¸Šï¼Œä¸ºæ¨¡å‹æä¾›é¡ºåºä¿¡æ¯ã€‚æ•´ä½“ç»“æ„ç”±ç¼–ç å™¨å’Œè§£ç å™¨å †å è€Œæˆï¼Œ " +
            "æ¯ä¸€å±‚éƒ½åŒ…å«å¤šå¤´è‡ªæ³¨æ„åŠ›ã€å‰é¦ˆç½‘ç»œã€æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–ã€‚è¯¥è®¾è®¡å¤§å¹…æå‡äº†è®­ç»ƒå¹¶è¡Œåº¦ï¼Œ " +
            "æˆä¸ºåç»­ BERTã€GPT ç³»åˆ—ä»¥åŠ T5 ç­‰æ¨¡å‹çš„åŸºç¡€ï¼Œæ¨åŠ¨äº†é¢„è®­ç»ƒåŠ å¾®è°ƒçš„æ–°èŒƒå¼ã€‚\n\n" +
            "äººç±»ï¼šå®ƒä¸ä¼ ç»Ÿ RNN ç›¸æ¯”æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ\n\n" +
            "åŠ©æ‰‹ï¼šæœ€ä¸»è¦çš„ä¼˜åŠ¿æ˜¯å¹¶è¡ŒåŒ–ã€‚RNN å¿…é¡»ä¾æ¬¡è®¡ç®—éšè—çŠ¶æ€ï¼Œè€Œ Transformer å¯ä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªåºåˆ—ï¼Œ " +
            "è®­ç»ƒé€Ÿåº¦æ˜¾è‘—æé«˜ã€‚åŒæ—¶ï¼Œè‡ªæ³¨æ„åŠ›ç›´æ¥å»ºæ¨¡ä»»æ„ä¸¤ä½ç½®é—´çš„ä¾èµ–ï¼Œç¼“è§£äº†é•¿è·ç¦»æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚";

        /* 3 ä¸­è‹±æ··åˆé•¿å¯¹è¯ï¼ˆæ—  special tokenï¼‰ */
        public static readonly string Mixed =
            "Userï¼šæœ€è¿‘å¤§æ¨¡å‹å¾ˆç«ï¼Œèƒ½ä¸èƒ½ç”¨ç®€å• English è§£é‡Šä¸€ä¸‹ RLHF æ˜¯æ€ä¹ˆåšçš„ï¼Ÿ\n\n" +
            "Assistantï¼šRLHF å…¨ç§° Reinforcement Learning from Human Feedbackï¼Œæ ¸å¿ƒæµç¨‹åˆ†ä¸‰æ­¥ã€‚ " +
            "ç¬¬ä¸€æ­¥ï¼Œç”¨ supervised fine-tuning åœ¨é«˜è´¨é‡äººå·¥æ ‡æ³¨æ•°æ®ä¸Šå¾®è°ƒ base æ¨¡å‹ï¼Œå¾—åˆ° SFT æ¨¡å‹ã€‚ " +
            "ç¬¬äºŒæ­¥ï¼Œæ”¶é›†åŒä¸€ prompt ä¸‹å¤šä¸ª response çš„å¯¹æ¯”æ•°æ®ï¼Œè®­ç»ƒä¸€ä¸ª reward model æ¥æ‰“åˆ†ã€‚ " +
            "ç¬¬ä¸‰æ­¥ï¼Œç”¨ reinforcement learningï¼ˆé€šå¸¸æ˜¯ PPOï¼‰ç»§ç»­ä¼˜åŒ– SFT æ¨¡å‹ï¼ŒæŠŠ reward model çš„åˆ†æ•°ä½œä¸º reward signalï¼Œ " +
            "åŒæ—¶åŠ å…¥ KL penalty é˜²æ­¢æ¨¡å‹åç¦»åŸå§‹åˆ†å¸ƒå¤ªè¿œã€‚è¿­ä»£å‡ è½®åï¼Œæ¨¡å‹å°±èƒ½è¾“å‡ºæ›´å¯¹é½äººç±»åå¥½çš„ç­”æ¡ˆã€‚ " +
            "æ•´ä¸ª pipeline éœ€è¦å¤§é‡äººå·¥æ ‡æ³¨å’Œè®¡ç®—èµ„æºï¼Œä½†æ•ˆæœä¸Šèƒ½æ˜¾è‘—é™ä½ harmful æˆ– untruthful è¾“å‡ºçš„æ¦‚ç‡ã€‚\n\n" +
            "Userï¼šè®­ç»ƒ reward model æ—¶æœ‰å“ªäº› tricksï¼Ÿ\n\n" +
            "Assistantï¼šå¸¸è§æŠ€å·§åŒ…æ‹¬ pair-wise æ’åºæŸå¤±ã€å¯¹åŒä¸€ batch å†…æ ·æœ¬åš normalizationã€ " +
            "ä»¥åŠä½¿ç”¨ larger batch size å’Œ lower learning rate æ¥ç¨³å®šè®­ç»ƒã€‚æ•°æ®è´¨é‡æ¯”æ•°é‡æ›´é‡è¦ï¼Œ " +
            "éœ€è¦ä¸¥æ ¼è¿‡æ»¤ inconsistent æˆ–æ¶æ„æ ‡æ³¨çš„æ ·æœ¬ã€‚";
    }

```

## Contribution

We welcome contributions to LumTokenizer. If you find any issues or have feature requests, please submit them through our issue tracker.

## License

LumDb is licensed under the MIT License. See the [LICENSE](LICENSE.txt) file for more information.

---

Please enjoy using LumTokenizer and help us make it even better by providing feedback and contributions!

